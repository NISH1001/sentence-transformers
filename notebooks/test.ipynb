{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef2c3644-59e2-404c-9475-8f854c25ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7e00c7-6927-416c-b74c-770130753a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to put the package in sys path\n",
    "# Alternate: make the package pip installable!\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a6d022-2f6a-433c-b3c4-deaa87f3ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d10c88c-3cc9-4b77-926f-d355c81c99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba5dd556-74f6-4db7-842f-9c3c7d82592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentform.modeling import SentenceTransformer\n",
    "from sentform.pooling import MeanPooling\n",
    "from sentform.utils import pairwise_cosine_similarity, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b211f80f-bcca-4611-a349-b5e12956765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d4ba8-9da4-4449-8f3f-8212ae2ece12",
   "metadata": {},
   "source": [
    "# SentenceTransformer Embeddings\n",
    "\n",
    "The `SentenceTransformer` is able to take in any backbone that is supported.\n",
    "In general, these backbones are BERT-based / BERT variants which give embeddings for each token.\n",
    "So, to get the embeddings for the whole sentence, we need a mechanism to aggregate these token embeddings.\n",
    "We can use `sentform.pooling.PoolingLayer` to do so. `MeanPooling` is a standard approach to aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc4bbca3-9ae5-40fb-808b-e78dddf5f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd08d871-d3f5-404c-8ba1-01bccc94c629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/npantha/dev/nish/projects/sentence-transformers/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentformer = SentenceTransformer(\n",
    "    backbone=backbone,\n",
    "    pooling_layer=MeanPooling()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12a3ed2c-bd33-4642-8bec-432a0b6ec455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (backbone): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pooling): MeanPooling()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54e73097-a75a-4fd9-ae33-4d77a8a038c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentformer.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "935ee134-a820-46bc-904c-dd25d46d68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I love cats.\",\n",
    "    \"I don't like mangoes.\",\n",
    "    \"They are using NLP in the company Fetch.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb4a6407-1b99-45d1-a213-2fcb1a7bd926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = sentformer.encode(sentences)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e575405-b0c8-461a-85b1-a95abdcb3918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5344,  0.3247, -0.1033,  ..., -0.0295,  0.2302,  0.2154],\n",
       "        [ 0.2443,  0.2077, -0.2987,  ...,  0.1340,  0.0335, -0.0820],\n",
       "        [ 0.0744, -0.1423,  0.2127,  ..., -0.4782,  0.1212,  0.1719]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daafe767-de13-49ab-b4f9-99756c502564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.7191, 0.4666],\n",
       "        [0.7191, 1.0000, 0.4731],\n",
       "        [0.4666, 0.4731, 1.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity-check similarity\n",
    "pairwise_cosine_similarity(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33537ed8-a640-4c2a-b259-27baa2c7a3e9",
   "metadata": {},
   "source": [
    "# Multi-Task learner\n",
    "\n",
    "Here, we implement `MultiTaskFormer` which takes in any backbone mentioned in the previous section.\n",
    "Plus, it also takes arbitrary number of `NetworkHead`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a3455be-951c-4b4b-abe1-2e7064cb5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentform.modeling import MultiTaskFormer\n",
    "from sentform.heads import ClassificationHead, NERHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6223278-1413-4d4a-94fb-c87f235919cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs fine-tuning of these heads\n",
    "# Left the tuning part for brevity as per assignment\n",
    "multi_tasker = MultiTaskFormer(\n",
    "    heads=[\n",
    "        ClassificationHead(\n",
    "            backbone.config.hidden_size,\n",
    "            num_classes=3,\n",
    "            labels=[\"Positive\", \"Neutral\", \"Negative\"],\n",
    "            multi_label=False\n",
    "        ),\n",
    "        NERHead(\n",
    "            backbone.config.hidden_size,\n",
    "            num_tags=3,\n",
    "            ner_tags=[\"Person\", \"Organization\", \"Location\"],\n",
    "            multi_label=False\n",
    "        )\n",
    "    ],\n",
    "    backbone=backbone,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c857fab-a9a6-42b5-8912-bc3c71f6cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = multi_tasker(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91f576a0-c774-4685-870d-b65d89d5492f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'head_0': {'logits': tensor([[-0.0638, -0.1983,  0.0074],\n",
       "          [ 0.0208,  0.1266,  0.2032],\n",
       "          [-0.0508,  0.1157,  0.0638]]),\n",
       "  'predicted_labels': ['Negative', 'Negative', 'Neutral']},\n",
       " 'head_1': {'logits': tensor([[[-0.3314, -0.5311,  0.1053],\n",
       "           [-0.2897, -0.6716,  0.2112],\n",
       "           [-0.1475, -0.8265, -0.1555],\n",
       "           [ 0.1923, -0.0331,  0.1125],\n",
       "           [-0.6644, -0.6219, -0.0615],\n",
       "           [-0.2435,  0.4752, -0.0369],\n",
       "           [-0.0547, -0.5965, -0.1171],\n",
       "           [-0.0953, -0.5887, -0.1721],\n",
       "           [ 0.0295, -0.5246, -0.1318],\n",
       "           [-0.1297, -0.5958, -0.2530],\n",
       "           [-0.0790, -0.5677, -0.1527],\n",
       "           [-0.0513, -0.5438, -0.0992]],\n",
       "  \n",
       "          [[-0.3009, -0.4976,  0.1998],\n",
       "           [-0.2441, -0.5311,  0.2400],\n",
       "           [-0.3107, -0.2357, -0.3953],\n",
       "           [-0.2541,  0.2635,  0.0801],\n",
       "           [-0.1912, -0.3858, -0.3256],\n",
       "           [-0.0159, -0.2862, -0.0302],\n",
       "           [-0.3075, -0.0702,  0.3962],\n",
       "           [-0.2758,  0.1982,  0.1446],\n",
       "           [-0.4386, -0.3423, -0.0923],\n",
       "           [-0.1159,  0.0474, -0.0023],\n",
       "           [-0.1556, -0.5621, -0.0545],\n",
       "           [-0.2751, -0.7029, -0.1213]],\n",
       "  \n",
       "          [[-0.3553, -0.4153,  0.2557],\n",
       "           [-0.5644, -0.2095,  0.1845],\n",
       "           [-0.5335, -0.1052,  0.2065],\n",
       "           [-0.3325, -0.0078, -0.1156],\n",
       "           [-0.5451, -0.2389,  0.2108],\n",
       "           [-0.1052, -0.0777, -0.4317],\n",
       "           [-0.0275, -0.4303,  0.0639],\n",
       "           [-0.4370, -0.3666,  0.2235],\n",
       "           [-0.0432, -0.1246, -0.3537],\n",
       "           [-0.7474, -0.3369,  0.0270],\n",
       "           [-0.3570,  0.3229, -0.0076],\n",
       "           [-0.0948,  0.2846,  0.1393]]]),\n",
       "  'predicted_labels': [['Location',\n",
       "    'Location',\n",
       "    'Person',\n",
       "    'Person',\n",
       "    'Location',\n",
       "    'Organization'],\n",
       "   ['Location',\n",
       "    'Location',\n",
       "    'Organization',\n",
       "    'Organization',\n",
       "    'Person',\n",
       "    'Person',\n",
       "    'Location',\n",
       "    'Organization',\n",
       "    'Location',\n",
       "    'Organization'],\n",
       "   ['Location',\n",
       "    'Location',\n",
       "    'Location',\n",
       "    'Organization',\n",
       "    'Location',\n",
       "    'Organization',\n",
       "    'Location',\n",
       "    'Location',\n",
       "    'Person',\n",
       "    'Location',\n",
       "    'Organization',\n",
       "    'Organization']]}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1111e27d-8bb6-42fd-ac7f-388e4a699dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I love cats.\n",
      "head_0 | Labels: Negative | Logits shape: torch.Size([3])\n",
      "head_1 | Labels: ['Location', 'Location', 'Person', 'Person', 'Location', 'Organization'] | Logits shape: torch.Size([12, 3])\n",
      "-------\n",
      "Sentence: I don't like mangoes.\n",
      "head_0 | Labels: Negative | Logits shape: torch.Size([3])\n",
      "head_1 | Labels: ['Location', 'Location', 'Organization', 'Organization', 'Person', 'Person', 'Location', 'Organization', 'Location', 'Organization'] | Logits shape: torch.Size([12, 3])\n",
      "-------\n",
      "Sentence: They are using NLP in the company Fetch.\n",
      "head_0 | Labels: Neutral | Logits shape: torch.Size([3])\n",
      "head_1 | Labels: ['Location', 'Location', 'Location', 'Organization', 'Location', 'Organization', 'Location', 'Location', 'Person', 'Location', 'Organization', 'Organization'] | Logits shape: torch.Size([12, 3])\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    for head_key, head_output in outputs.items():\n",
    "        predicted_labels = head_output[\"predicted_labels\"][i]\n",
    "        logits_shape = head_output[\"logits\"][i].shape\n",
    "        print(f\"{head_key} | Labels: {predicted_labels} | Logits shape: {logits_shape}\")\n",
    "    print(\"-\" * 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84790d32-41fb-4096-94d0-03a385e200ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f705cc-52e0-4eaf-8146-31f0123fda32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
